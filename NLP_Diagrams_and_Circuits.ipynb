{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 12 processes.\n"
     ]
    }
   ],
   "source": [
    "from lambeq import BobcatParser, SpacyTokeniser, Rewriter, AtomicType, IQPAnsatz\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "import os, time, multiprocessing\n",
    "\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab/french')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab/french')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\" #environment variable for multithreading\n",
    "\n",
    "#Global data sequencing variables\n",
    "parser = BobcatParser(verbose=\"suppress\")\n",
    "tokeniser = SpacyTokeniser()\n",
    "rewriter = Rewriter(['prepositional_phrase', 'determiner']) #potentially add more rules for rewrite\n",
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "ansatz = IQPAnsatz({N:1,S:1}, n_layers=2, n_single_qubit_params=3)\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "print(f\"Using {num_processes} processes.\")\n",
    "\n",
    "def process_sentence(sentence: str, is_french: bool = False) -> Optional[object]:\n",
    "    \"\"\"\n",
    "    A single sentence process, designed to be the base function for each process\n",
    "    Args: String to process\n",
    "    Returns: A semantic diagram of the sentence, or nothing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sentence = sentence.lower() #make all lowercase\n",
    "        if is_french:\n",
    "             tokens = word_tokenize(sentence, language='french') #switch english to ntlk tokenizer instead of spaCy?\n",
    "             return tokens\n",
    "            # return diagram #change this line\n",
    "        else:\n",
    "            #sentence = tokeniser.split_sentences(sentence) #for more complex sentences this will be needed, but breaks single sentence itterances\n",
    "            tokens = tokeniser.tokenise_sentence(sentence)\n",
    "            diagram = parser.sentence2diagram(tokens, tokenised=True)\n",
    "            rewritten_diagram = rewriter(diagram)\n",
    "            normalised_diagram = rewritten_diagram.normal_form()\n",
    "            return normalised_diagram\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse: {sentence}\")\n",
    "        return None\n",
    "\n",
    "def process_sentences_batch(sentences: List[str], is_french: bool)-> List[Optional[object]]:\n",
    "    \"\"\"\n",
    "    Processes a batch of sentences\n",
    "    Args: List of sentences, is_french\n",
    "    Returns: List of diagrams\n",
    "    \"\"\"\n",
    "    return [process_sentence(sentence, is_french) for sentence in sentences]\n",
    "\n",
    "def process_sentences(sentences: List[str], batch_size: int = 200, is_french: bool = False) -> Dict[int,Optional[object]]:\n",
    "    \"\"\"\n",
    "    Processes sentences in batches of 200(default only) in a parallel structure\n",
    "    Args: sentences, list of strings\n",
    "          batch_size number of sentences per batch\n",
    "          is_french, flag set for second lang\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary of processed diagrams or none\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    diagrams: Dict[int, Optional[object]] = {}\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        indexed_sentences = list(enumerate(sentences))\n",
    "        batches = [indexed_sentences[i:i + batch_size] for i in range(0, len(indexed_sentences), batch_size)]\n",
    "        results = pool.map(lambda batch: {i: process_sentence(sentence, is_french) for i, sentence in batch}, batches) #lambda function to pass flag\n",
    "        for result_dict in results:\n",
    "            diagrams.update(result_dict)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    print(f\"Processed {len(sentences)} sentences in {end_time - start_time: .2f} seconds using a batch size of {batch_size}.\")\n",
    "    return diagrams\n",
    "\n",
    "def read_sentences_from_file(filename: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads sentences from file\n",
    "    Args: Filename, str\n",
    "    Returns: list of sentences\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            sentences = [line.strip() for line in f]\n",
    "        return sentences\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Wrong file name\")\n",
    "        return [] #return Nullset for list item\n",
    "    \n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"missed utf-8 encoding\")\n",
    "        return []\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unknown Error\")\n",
    "        return []\n",
    "    \n",
    "def convert_diagram(diagram:Optional[object]) -> Optional[object]:\n",
    "    \"\"\"Converts a diagram to it's equivilant quantum circuit\n",
    "\n",
    "    Args:\n",
    "        diagram (Optional[object]): Diagram Obj\n",
    "\n",
    "    Returns:\n",
    "        Optional[object]: Circuit obj, or None\n",
    "    \"\"\"\n",
    "    if diagram is None:\n",
    "        return None\n",
    "    try:\n",
    "        circuit = ansatz(diagram)\n",
    "        return circuit\n",
    "    except Exception as e:\n",
    "        print(\"Failed to convert diagram to circuit\")\n",
    "        return None\n",
    "    \n",
    "def convert_diagrams_batch(diagrams: List[Optional[object]]) -> List[Optional[object]]:\n",
    "    return [convert_diagram(diagram) for diagram in diagrams]\n",
    "\n",
    "def process_diagrams(diagrams: Dict[int, Optional[object]], batch_size: int = 200) -> Dict[int, Optional[object]]:\n",
    "    \"\"\"Converts diagrams to circuits in batches parallelizing the work, while maintaining original sentence indicies\n",
    "\n",
    "    Args:\n",
    "        diagrams: Dictionary mapping sentence indices to diagram for efficent storage\n",
    "        batch_size (int, optional): number of diagrams per batch\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, Optional[object]]: _description_A dictionary mapping sentence indicies to circuits\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    indicies = list(diagrams.keys())\n",
    "    batched_indices = [indicies[i:i + batch_size] for i in range(0, len(indicies), batch_size)]\n",
    "    \n",
    "    diagram_batches = [[diagrams[index] for index in index_batch] for index_batch in batched_indices]\n",
    "    with multiprocessing.pool(num_processes) as pool:\n",
    "        circuit_batches = pool.map(convert_diagrams_batch, diagram_batches)\n",
    "        \n",
    "    circuits: Dict[int,Optional[object]] = {}\n",
    "    for i, index_batch in enumerate(batched_indices):\n",
    "        for j, index in enumerate(index_batch):\n",
    "                circuits[index] = circuit_batches[i][j]\n",
    "    end_time = time.time()\n",
    "    print(f\"Converted {len(diagrams)} diagrams to circuits in {end_time - start_time:.2f} seconds using batch size of {batch_size}.\")\n",
    "    return circuits\n",
    "\n",
    "def save_circuit(circuit: object, filename:str) -> None:\n",
    "    \"\"\"Saves a single diagram to a file.\"\"\"\n",
    "    try:\n",
    "        if circuit is None:\n",
    "            print(f\"Warning Object missing, Skipping {filename}.\")\n",
    "            return\n",
    "        else:\n",
    "            circuit.draw(path=filename, figsize=(20,20), draw_type_labels=False)\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving {filename}: {e}\")\n",
    "        \n",
    "def create_circuit_pairs(english_circuits: Dict[int, Optional[object]], french_circuits: Dict[int, Optional[object]]) -> List[Tuple[Optional[object], Optional[object]]]:\n",
    "    \"\"\"\n",
    "    Pairs English and french circuits based on their original sentence indices.\n",
    "\n",
    "    Args:\n",
    "        english_circuits: Dictionary mapping English sentence indices to circuits.\n",
    "        french_circuits: Dictionary mapping french sentence indices to circuits.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains an (English circuit, french circuit) pair.\n",
    "        Only pairs with matching indices are included.  If a circuit is missing for a given\n",
    "        index in either language, that index is skipped.\n",
    "    \"\"\"\n",
    "    circuit_pairs: List[Tuple[Optional[object], Optional[object]]] = []\n",
    "    common_indices = sorted(list(english_circuits.keys() & french_circuits.keys()))  # Get sorted common indices\n",
    "\n",
    "    for index in common_indices:\n",
    "        en_circuit = english_circuits.get(index)\n",
    "        fr_circuit = french_circuits.get(index)\n",
    "        circuit_pairs.append((en_circuit, fr_circuit))\n",
    "\n",
    "    return circuit_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  en  \\\n",
      "0  Changing Lives | Changing Society | How It Wor...   \n",
      "1                                           Site map   \n",
      "2                                           Feedback   \n",
      "3                                            Credits   \n",
      "4                                           Français   \n",
      "\n",
      "                                                  fr  \n",
      "0  Il a transformé notre vie | Il a transformé la...  \n",
      "1                                       Plan du site  \n",
      "2                                        Rétroaction  \n",
      "3                                            Crédits  \n",
      "4                                            English  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   en      9999 non-null   object\n",
      " 1   fr      10000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 156.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "chunk_iterator = pd.read_csv(r'C:/Users/Jash/Documents/Research/QNLP/en-fr.csv', chunksize=10000)\n",
    "first_chunk = next(chunk_iterator)\n",
    "print(first_chunk.head())\n",
    "print(first_chunk.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 128 processes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse: join us, won't you?\n",
      "Failed to parse: join us, won't you?\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if __name__ == \"__main__\": #guard \n",
    "        multiprocessing.freeze_support() #windows support, not needed for linux line\n",
    "        english_sentences = read_sentences_from_file(\"english.txt\")\n",
    "        #french_sentences = read_sentences_from_file(\"french.txt\")\n",
    "    \n",
    "        english_diagrams = process_sentences(english_sentences, batch_size=200)\n",
    "        #french_diagrams = process_sentences(french_sentences, batch_size=200, is_french=True)\n",
    "\n",
    "        english_circuits = process_diagrams(english_diagrams)\n",
    "        #french_circuits = process_diagrams(french_diagrams)\n",
    "        \n",
    "        for index, circuit in english_circuits.items():\n",
    "            if circuit:\n",
    "                filename = f\"english_circuit_{index + 1}.png\"\n",
    "                save_circuit(circuit, filename)\n",
    "            \n",
    "        # for index, circuit in french_circuits.items():\n",
    "        #     if circuit:\n",
    "        #         filename = f\"french_circuit_{index + 1}.png\"\n",
    "\n",
    "        #ml_data = create_circuit_pairs(english_circuits,french_circuits)\n",
    "        print(f\"Number of English circuits: {len(english_circuits)}\")\n",
    "        #print(f\"Number of french circuits: {len(french_circuits)}\")\n",
    "        #print(f\"Number of matching circuit pairs: {len(ml_data)}\")\n",
    "        #print(f\"Data for machine learning (first 10 pairs): {ml_data[:10]}\")\n",
    "\n",
    "\n",
    "#find correct batch size for time constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug cell\n",
    "test_diagram = process_sentence(\"Hi, how are you doing today?\")\n",
    "#test_diagram.draw(figsize=(14,3), fontsize=12)\n",
    "test_circuit = convert_diagram(test_diagram)\n",
    "#test_circuit.draw(figsize=(14,3), fontsize=12)\n",
    "#english_diagrams[15562].draw(figsize=(14,3), fontsize=12)\n",
    "save_circuit(test_circuit, \"test_save.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.1\n",
    "SEED = 42\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_hat, y):\n",
    "    return (torch.argmax(y_hat, dim=1) ==\n",
    "            torch.argmax(y, dim=1)).sum().item()/len(y)\n",
    "\n",
    "def loss(y_hat, y):\n",
    "    return torch.nn.functional.mse_loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import PennyLaneModel\n",
    "\n",
    "\n",
    "backend_config = {'backend': 'default.qubit'}\n",
    "#model = PennyLaneModel.from_diagrams(ml_data, probabilities=True, normalize=True,backend_config=backend_config)\n",
    "#model.initialise_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import PytorchTrainer\n",
    "\n",
    "trainer = PytorchTrainer(\n",
    "    #model=model,\n",
    "    loss_function=loss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    epochs=EPOCHS,\n",
    "    evaluate_functions={'acc': acc},\n",
    "    evaluate_on_train=True,\n",
    "    use_tensorboard=False,\n",
    "    verbose='text',\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "qml.default_config['qiskit.ibmq.ibmqx_token'] = 'my_API_token'\n",
    "qml.default_config.save(qml.default_config.path)\n",
    "backend_config = {'backend': 'qiskit.ibmq',\n",
    "                  'device': 'ibmq_manilia',\n",
    "                  'shots': 1000}\n",
    "#q_model = PennyLaneModel.from_diagrams(ml_data, probabilities=True, normalize=True, backend_config=backend_config)\n",
    "#q_model = initialise_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trainer.fit(train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

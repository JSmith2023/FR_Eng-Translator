{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization, Lemmatization and POS tagging complete.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "def process_sentences(filename, nlp):\n",
    "    \"\"\"Tokenizes, Lemmatizes, and tags sentences from a file.\"\"\"\n",
    "    processed_data = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                doc = nlp(line)\n",
    "                sentence_data = [(token.lemma_, token.pos_) for token in doc]\n",
    "                processed_data.append(sentence_data)\n",
    "    return processed_data\n",
    "\n",
    "# Process files\n",
    "english_sentences = process_sentences(\"english.txt\", nlp_en)\n",
    "russian_sentences = process_sentences(\"russian.txt\", nlp_ru)\n",
    "\n",
    "#Save the processed data to a file.\n",
    "def save_processed_sentences(data, filename):\n",
    "    with open(filename, \"w\", encoding = \"utf-8\") as f:\n",
    "        for sentence in data:\n",
    "            f.write(str(sentence) + \"\\n\")\n",
    "\n",
    "save_processed_sentences(english_sentences, \"english_tagged.txt\")\n",
    "save_processed_sentences(russian_sentences, \"russian_tagged.txt\")\n",
    "\n",
    "print(\"Tokenization, Lemmatization and POS tagging complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse: Isn't that so?\n",
      "Failed to parse: Isn't that so?\n",
      "Failed to parse: Aren't you hot?\n",
      "Failed to parse: Aren't you hot?\n",
      "Failed to parse: Aren't you sad?\n",
      "Failed to parse: Aren't you sad?\n",
      "Failed to parse: Can't Tom swim?\n",
      "Failed to parse: Can't you read?\n",
      "Failed to parse: Can't you read?\n",
      "Failed to parse: Can't you read?\n",
      "Failed to parse: Can't you read?\n",
      "Failed to parse: Can't you sing?\n",
      "Failed to parse: Can't you sing?\n",
      "Failed to parse: Can't you sing?\n",
      "Failed to parse: Can't you sing?\n",
      "Failed to parse: Can't you swim?\n",
      "Failed to parse: Can't you swim?\n",
      "Failed to parse: Didn't Tom cry?\n",
      "Failed to parse: Didn't Tom win?\n",
      "Failed to parse: Didn't Tom win?\n",
      "Failed to parse: Don't you care?\n",
      "Failed to parse: Don't you care?\n",
      "Failed to parse: Don't you know?\n",
      "Failed to parse: Don't you know?\n",
      "Failed to parse: Don't you know?\n",
      "Failed to parse: Don't you know?\n",
      "Failed to parse: Isn't it black?\n",
      "Failed to parse: Isn't it black?\n",
      "Failed to parse: Isn't it black?\n",
      "Failed to parse: Isn't it great?\n",
      "Failed to parse: Isn't it weird?\n",
      "Failed to parse: Isn't that Tom?\n",
      "Failed to parse: Isn't that odd?\n",
      "Failed to parse: Isn't that sad?\n",
      "Failed to parse: Isn't that sad?\n",
      "Failed to parse: Aren't we going?\n",
      "Failed to parse: Aren't we going?\n",
      "Failed to parse: Aren't you busy?\n",
      "Failed to parse: Aren't you busy?\n",
      "Failed to parse: Aren't you busy?\n",
      "Failed to parse: Aren't you glad?\n",
      "Failed to parse: Aren't you glad?\n",
      "Failed to parse: Aren't you glad?\n",
      "Failed to parse: Aren't you rich?\n",
      "Failed to parse: Aren't you rich?\n",
      "Failed to parse: Can't I come in?\n",
      "Failed to parse: Can't we go now?\n",
      "Failed to parse: Can't you do it?\n",
      "Failed to parse: Can't you do it?\n",
      "Failed to parse: Didn't Tom sing?\n",
      "Failed to parse: Didn't you know?\n",
      "Failed to parse: Didn't you know?\n",
      "Failed to parse: Didn't you sing?\n",
      "Failed to parse: Didn't you sing?\n",
      "Failed to parse: Doesn't it work?\n",
      "Failed to parse: Don't you agree?\n",
      "Failed to parse: Don't you agree?\n",
      "Failed to parse: Don't you agree?\n",
      "Failed to parse: I could've died.\n",
      "Failed to parse: I could've died.\n",
      "Failed to parse: I would've paid.\n",
      "Failed to parse: I would've paid.\n",
      "Failed to parse: Isn't that mine?\n",
      "Failed to parse: Isn't that ours?\n",
      "Failed to parse: Isn't that true?\n",
      "Failed to parse: Tom must've won.\n",
      "Failed to parse: Aren't you a cop?\n",
      "Failed to parse: Aren't you a cop?\n",
      "Failed to parse: Aren't you angry?\n",
      "Failed to parse: Aren't you happy?\n",
      "Failed to parse: Aren't you ready?\n",
      "Failed to parse: Aren't you ready?\n",
      "Failed to parse: Aren't you tired?\n",
      "Failed to parse: Aren't you tired?\n",
      "Failed to parse: Aren't you tired?\n",
      "Failed to parse: Aren't you tired?\n",
      "Failed to parse: Can't you decide?\n",
      "Failed to parse: Can't you decide?\n",
      "Failed to parse: Can't you decide?\n",
      "Failed to parse: Can't you decide?\n",
      "Failed to parse: Can't you decide?\n",
      "Failed to parse: Can't you decide?\n",
      "Failed to parse: Didn't Tom smile?\n",
      "Failed to parse: Didn't Tom smile?\n",
      "Failed to parse: Doesn't Tom swim?\n",
      "Failed to parse: Don't I know you?\n",
      "Failed to parse: Don't you get it?\n",
      "Failed to parse: Don't you get it?\n",
      "Failed to parse: Don't you see me?\n",
      "Failed to parse: Don't you see me?\n",
      "Failed to parse: Isn't Tom hungry?\n",
      "Failed to parse: Isn't Tom hungry?\n",
      "Failed to parse: Isn't Tom hungry?\n",
      "Failed to parse: Isn't it amazing?\n",
      "Failed to parse: Isn't it obvious?\n",
      "Failed to parse: Isn't it obvious?\n",
      "Failed to parse: Isn't it strange?\n",
      "Failed to parse: Isn't that risky?\n",
      "Failed to parse: Isn't that weird?\n",
      "Failed to parse: Isn't this great?\n",
      "Failed to parse: Tom must've died.\n",
      "Failed to parse: Tom must've left.\n",
      "Failed to parse: Tom must've left.\n",
      "Failed to parse: We could've died.\n",
      "Failed to parse: We could've died.\n",
      "Failed to parse: Why don't we sit?\n",
      "Failed to parse: Aren't we friends?\n",
      "Failed to parse: Aren't you afraid?\n",
      "Failed to parse: Aren't you afraid?\n",
      "Failed to parse: Aren't you afraid?\n",
      "Failed to parse: Aren't you afraid?\n",
      "Failed to parse: Aren't you hungry?\n",
      "Failed to parse: Aren't you hungry?\n",
      "Failed to parse: Aren't you hungry?\n",
      "Failed to parse: Aren't you hungry?\n",
      "Failed to parse: Aren't you hungry?\n",
      "Failed to parse: Aren't you hungry?\n",
      "Failed to parse: Aren't you hungry?\n",
      "Failed to parse: Aren't you hungry?\n",
      "Failed to parse: Aren't you hungry?\n",
      "Failed to parse: Aren't you sleepy?\n",
      "Failed to parse: Aren't you sleepy?\n",
      "Failed to parse: Aren't you sleepy?\n",
      "Failed to parse: Aren't you sleepy?\n",
      "Failed to parse: Can't we tell Tom?\n",
      "Failed to parse: Can't you do that?\n",
      "Failed to parse: Can't you do that?\n",
      "Failed to parse: Can't you do that?\n",
      "Failed to parse: Can't you do that?\n",
      "Failed to parse: Can't you feel it?\n",
      "Failed to parse: Can't you find it?\n",
      "Failed to parse: Can't you find it?\n",
      "Failed to parse: Can't you find it?\n",
      "Failed to parse: Can't you find it?\n",
      "Failed to parse: Can't you find it?\n",
      "Failed to parse: Can't you hear it?\n",
      "Failed to parse: Can't you hear it?\n",
      "Failed to parse: Can't you hear it?\n",
      "Failed to parse: Can't you hear it?\n",
      "Failed to parse: Can't you help me?\n",
      "Failed to parse: Can't you help me?\n",
      "Failed to parse: Can't you help us?\n",
      "Failed to parse: Can't you help us?\n",
      "Failed to parse: Can't you help us?\n",
      "Failed to parse: Can't you help us?\n",
      "Failed to parse: Couldn't Tom help?\n",
      "Failed to parse: Couldn't Tom help?\n",
      "Failed to parse: Didn't I say that?\n",
      "Failed to parse: Didn't I say that?\n",
      "Failed to parse: Didn't I tell you?\n",
      "Failed to parse: Didn't I tell you?\n",
      "Failed to parse: Didn't I tell you?\n",
      "Failed to parse: Didn't I tell you?\n",
      "Failed to parse: Didn't I warn you?\n",
      "Failed to parse: Didn't I warn you?\n",
      "Failed to parse: Didn't you go out?\n",
      "Failed to parse: Didn't you go out?\n",
      "Failed to parse: Didn't you go out?\n",
      "Failed to parse: Didn't you go out?\n",
      "Failed to parse: Didn't you see it?\n",
      "Failed to parse: Didn't you see it?\n",
      "Failed to parse: Didn't you see it?\n",
      "Failed to parse: Didn't you see it?\n",
      "Failed to parse: Don't I get a hug?\n",
      "Failed to parse: Don't I get a hug?\n",
      "Failed to parse: Don't you feel it?\n",
      "Failed to parse: Don't you feel it?\n",
      "Failed to parse: Don't you feel it?\n",
      "Failed to parse: Don't you hear it?\n",
      "Failed to parse: Don't you hear it?\n",
      "Failed to parse: Don't you hear it?\n",
      "Failed to parse: Don't you hear it?\n",
      "Failed to parse: Don't you hear it?\n",
      "Failed to parse: Don't you know me?\n",
      "Failed to parse: Don't you know me?\n",
      "Failed to parse: Don't you know me?\n",
      "Failed to parse: Don't you know me?\n",
      "Failed to parse: Don't you like it?\n",
      "Failed to parse: Don't you like it?\n",
      "Failed to parse: Don't you like me?\n",
      "Failed to parse: Don't you like me?\n",
      "Failed to parse: Don't you like us?\n",
      "Failed to parse: Don't you like us?\n",
      "Failed to parse: Don't you miss me?\n",
      "Failed to parse: Don't you miss me?\n",
      "Failed to parse: Don't you see Tom?\n",
      "Failed to parse: Don't you see Tom?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissed utf-8 encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#     with open(\"russian.txt\", \"r\") as f:\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#         for line in f:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# russian_diagrams = process_diagrams(russian_sentences)\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mprocess_diagrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43menglish_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menglish_diagrams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#process_diagrams(\"russian.txt\", russian_diagrams)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m, in \u001b[0;36mprocess_diagrams\u001b[1;34m(sentences, diagrams)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#sentence = tokeniser.split_sentences(sentence) #for more complex sentences this will be needed, but breaks single sentence itterances\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokeniser\u001b[38;5;241m.\u001b[39mtokenise_sentence(sentence)\n\u001b[1;32m---> 11\u001b[0m     diagram \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence2diagram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#rewrite rule for prepositional phrases\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     rewritten_diagram \u001b[38;5;241m=\u001b[39m rewriter(diagram)\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\text2diagram\\ccg_parser.py:232\u001b[0m, in \u001b[0;36mCCGParser.sentence2diagram\u001b[1;34m(self, sentence, tokenised, planar, collapse_noun_phrases, suppress_exceptions)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`tokenised` set to `True`, but variable \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`sentence` does not have type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    230\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`list[str]`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    231\u001b[0m     sent: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sentence]\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences2diagrams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m[\u001b[49m\u001b[43msent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mplanar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplanar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcollapse_noun_phrases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollapse_noun_phrases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msuppress_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtokenised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVerbosityLevel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSUPPRESS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentence, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\text2diagram\\ccg_parser.py:163\u001b[0m, in \u001b[0;36mCCGParser.sentences2diagrams\u001b[1;34m(self, sentences, tokenised, planar, collapse_noun_phrases, suppress_exceptions, verbose)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences2diagrams\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    124\u001b[0m                        sentences: SentenceBatchType,\n\u001b[0;32m    125\u001b[0m                        tokenised: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m                        suppress_exceptions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    129\u001b[0m                        verbose: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Diagram \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse multiple sentences into a list of lambeq diagrams.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     trees \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences2trees\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43msuppress_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuppress_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtokenised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     diagrams: \u001b[38;5;28mlist\u001b[39m[Diagram \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\text2diagram\\bobcat_parser.py:280\u001b[0m, in \u001b[0;36mBobcatParser.sentences2trees\u001b[1;34m(self, sentences, tokenised, suppress_exceptions, verbose)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m==\u001b[39m VerbosityLevel\u001b[38;5;241m.\u001b[39mTEXT\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTagging sentences.\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m--> 280\u001b[0m tag_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m tags \u001b[38;5;241m=\u001b[39m tag_results\u001b[38;5;241m.\u001b[39mtags\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m==\u001b[39m VerbosityLevel\u001b[38;5;241m.\u001b[39mTEXT\u001b[38;5;241m.\u001b[39mvalue:\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\bobcat\\tagger.py:402\u001b[0m, in \u001b[0;36mTagger.__call__\u001b[1;34m(self, inputs, batch_size, verbose)\u001b[0m\n\u001b[0;32m    394\u001b[0m output \u001b[38;5;241m=\u001b[39m TaggerOutput(tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtags,\n\u001b[0;32m    395\u001b[0m                       cats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcats,\n\u001b[0;32m    396\u001b[0m                       sentences\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(inputs), batch_size,\n\u001b[0;32m    399\u001b[0m                 desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTagging sentences\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    400\u001b[0m                 leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    401\u001b[0m                 disable\u001b[38;5;241m=\u001b[39mverbose \u001b[38;5;241m!=\u001b[39m VerbosityLevel\u001b[38;5;241m.\u001b[39mPROGRESS\u001b[38;5;241m.\u001b[39mvalue):\n\u001b[1;32m--> 402\u001b[0m     output\u001b[38;5;241m.\u001b[39msentences\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\bobcat\\tagger.py:333\u001b[0m, in \u001b[0;36mTagger.parse\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse a batch of sentences.\"\"\"\u001b[39;00m\n\u001b[0;32m    332\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs(inputs, word_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 333\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencodings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m tag_output: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[TagListT]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    337\u001b[0m span_output: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[TagListT]] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\bobcat\\tagger.py:132\u001b[0m, in \u001b[0;36mBertForChartClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, tag_labels, span_labels, word_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    116\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m     return_dict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    128\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChartClassifierOutput \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m    129\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m (return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    130\u001b[0m                    \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict)\n\u001b[1;32m--> 132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;66;03m# remove ignored tensors and pack remaining ones\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pytorch_utils.py:261\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:554\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    552\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    553\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m--> 554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:2910\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2902\u001b[0m         layer_norm,\n\u001b[0;32m   2903\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2908\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2909\u001b[0m     )\n\u001b[1;32m-> 2910\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2911\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2912\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lambeq import BobcatParser, SpacyTokeniser, AtomicType, IQPAnsatz, Rewriter\n",
    "\n",
    "parser = BobcatParser(verbose=\"suppress\")\n",
    "tokeniser = SpacyTokeniser()\n",
    "rewriter = Rewriter(['prepositional_phrase', 'determiner'])\n",
    "def process_diagrams(sentences, diagrams):\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            #sentence = tokeniser.split_sentences(sentence) #for more complex sentences this will be needed, but breaks single sentence itterances\n",
    "            tokens = tokeniser.tokenise_sentence(sentence)\n",
    "            diagram = parser.sentence2diagram(tokens, tokenised=True)\n",
    "            #rewrite rule for prepositional phrases\n",
    "            rewritten_diagram = rewriter(diagram)\n",
    "            #normalize diagram\n",
    "            normalised_diagram = rewritten_diagram.normal_form() #explore more rewriting ideas\n",
    "            diagrams.append(normalised_diagram)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse: {sentence}\")\n",
    "\n",
    "english_diagrams = []\n",
    "russian_diagrams = []\n",
    "english_sentences = []\n",
    "russian_sentences = []\n",
    "\n",
    "try:\n",
    "    with open(\"english.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            english_sentences.append(line.strip())\n",
    "except FileNotFoundError:\n",
    "    print(\"Wrong file name\")\n",
    "except UnicodeDecodeError:\n",
    "    print(\"missed utf-8 encoding\")\n",
    "\n",
    "# try:\n",
    "#     with open(\"russian.txt\", \"r\") as f:\n",
    "#         for line in f:\n",
    "#             russian_sentences.append(line.strip())\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Error: 'russian.txt' not found.\")\n",
    "#\n",
    "# russian_diagrams = process_diagrams(russian_sentences)\n",
    "\n",
    " \n",
    "process_diagrams(english_sentences, english_diagrams)\n",
    "#process_diagrams(\"russian.txt\", russian_diagrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#append to file after every 20 diagrams\n",
    "english_diagrams[15562].draw(figsize=(14,3), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Ty(p)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         circuit \u001b[38;5;241m=\u001b[39m ansatz(diagram)\n\u001b[0;32m     12\u001b[0m         circuits\u001b[38;5;241m.\u001b[39mappend(circuit)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mprocess_circuits\u001b[49m\u001b[43m(\u001b[49m\u001b[43menglish_diagrams\u001b[49m\u001b[43m,\u001b[49m\u001b[43menglish_circuits\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m, in \u001b[0;36mprocess_circuits\u001b[1;34m(diagrams, circuits)\u001b[0m\n\u001b[0;32m      8\u001b[0m ansatz \u001b[38;5;241m=\u001b[39m IQPAnsatz({N:\u001b[38;5;241m1\u001b[39m, S:\u001b[38;5;241m1\u001b[39m}, n_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m diagram \u001b[38;5;129;01min\u001b[39;00m diagrams:\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#Convert string Diagram to quantum circuit\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     circuit \u001b[38;5;241m=\u001b[39m \u001b[43mansatz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiagram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     circuits\u001b[38;5;241m.\u001b[39mappend(circuit)\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\ansatz\\circuit.py:114\u001b[0m, in \u001b[0;36mCircuitAnsatz.__call__\u001b[1;34m(self, diagram)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, diagram: Diagram) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Circuit:\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a lambeq diagram into a lambeq circuit.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiagram\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\backend\\grammar.py:1927\u001b[0m, in \u001b[0;36mFunctor.__call__\u001b[1;34m(self, entity)\u001b[0m\n\u001b[0;32m   1925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_with_cache(entity)\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mar_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\backend\\grammar.py:1952\u001b[0m, in \u001b[0;36mFunctor.ar_with_cache\u001b[1;34m(self, ar)\u001b[0m\n\u001b[0;32m   1949\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ar\u001b[38;5;241m.\u001b[39mis_id:\n\u001b[1;32m-> 1952\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_functor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1954\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_category\u001b[38;5;241m.\u001b[39mDiagram\u001b[38;5;241m.\u001b[39mid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_with_cache(ar\u001b[38;5;241m.\u001b[39mdom))\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\backend\\grammar.py:1219\u001b[0m, in \u001b[0;36mDiagram.apply_functor\u001b[1;34m(self, functor)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m   1217\u001b[0m     left, box, right \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39munpack()\n\u001b[0;32m   1218\u001b[0m     diagram \u001b[38;5;241m>>\u001b[39m\u001b[38;5;241m=\u001b[39m (functor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid(left))\n\u001b[1;32m-> 1219\u001b[0m                  \u001b[38;5;241m@\u001b[39m \u001b[43mfunctor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_diagram()\n\u001b[0;32m   1220\u001b[0m                  \u001b[38;5;241m@\u001b[39m functor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid(right)))\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m diagram\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\backend\\grammar.py:1927\u001b[0m, in \u001b[0;36mFunctor.__call__\u001b[1;34m(self, entity)\u001b[0m\n\u001b[0;32m   1925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_with_cache(entity)\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mar_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\backend\\grammar.py:1952\u001b[0m, in \u001b[0;36mFunctor.ar_with_cache\u001b[1;34m(self, ar)\u001b[0m\n\u001b[0;32m   1949\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ar\u001b[38;5;241m.\u001b[39mis_id:\n\u001b[1;32m-> 1952\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_functor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1954\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_category\u001b[38;5;241m.\u001b[39mDiagram\u001b[38;5;241m.\u001b[39mid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_with_cache(ar\u001b[38;5;241m.\u001b[39mdom))\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\backend\\grammar.py:451\u001b[0m, in \u001b[0;36mBox.apply_functor\u001b[1;34m(self, functor)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munwind())\u001b[38;5;241m.\u001b[39mrotate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz)\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\backend\\grammar.py:1980\u001b[0m, in \u001b[0;36mFunctor.ar\u001b[1;34m(self, ar)\u001b[0m\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpecify a custom ar function if you want to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1978\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse the functor on boxes.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1980\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_ar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\ansatz\\circuit.py:129\u001b[0m, in \u001b[0;36mCircuitAnsatz._ar\u001b[1;34m(self, _, box)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ar\u001b[39m(\u001b[38;5;28mself\u001b[39m, _: Functor, box: Box) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Circuit:\n\u001b[0;32m    128\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarise_box(box)\n\u001b[1;32m--> 129\u001b[0m     dom, cod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_size(box\u001b[38;5;241m.\u001b[39mdom), \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mob_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     n_qubits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(dom, cod)\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_qubits \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\ansatz\\circuit.py:118\u001b[0m, in \u001b[0;36mCircuitAnsatz.ob_size\u001b[1;34m(self, pg_type)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mob_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, pg_type: Ty) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate the number of qubits used for a given type.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpg_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\backend\\grammar.py:1925\u001b[0m, in \u001b[0;36mFunctor.__call__\u001b[1;34m(self, entity)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Apply the functor to a type or a diagrammable.\u001b[39;00m\n\u001b[0;32m   1917\u001b[0m \n\u001b[0;32m   1918\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1922\u001b[0m \n\u001b[0;32m   1923\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(entity, Ty):\n\u001b[1;32m-> 1925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mob_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mar_with_cache(entity)\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\backend\\grammar.py:1939\u001b[0m, in \u001b[0;36mFunctor.ob_with_cache\u001b[1;34m(self, ob)\u001b[0m\n\u001b[0;32m   1937\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_category\u001b[38;5;241m.\u001b[39mTy()\n\u001b[0;32m   1938\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1939\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_functor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_cache[ob] \u001b[38;5;241m=\u001b[39m ret\n\u001b[0;32m   1942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\backend\\grammar.py:286\u001b[0m, in \u001b[0;36mTy.apply_functor\u001b[1;34m(self, functor)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munwind())\u001b[38;5;241m.\u001b[39mrotate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\backend\\grammar.py:1972\u001b[0m, in \u001b[0;36mFunctor.ob\u001b[1;34m(self, ob)\u001b[0m\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_ob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpecify a custom ob function if you want to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1971\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse the functor on types.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1972\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_ob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lambeq\\ansatz\\circuit.py:125\u001b[0m, in \u001b[0;36mCircuitAnsatz._ob\u001b[1;34m(self, _, ty)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ob\u001b[39m(\u001b[38;5;28mself\u001b[39m, _: Functor, ty: Ty) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Ty:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mob_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mty\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: Ty(p)"
     ]
    }
   ],
   "source": [
    "from lambeq import AtomicType, IQPAnsatz\n",
    "english_circuits = []\n",
    "russian_circuits = []\n",
    "def process_circuits(diagrams, circuits):\n",
    "    #Define atomic types\n",
    "    N = AtomicType.NOUN\n",
    "    S = AtomicType.SENTENCE\n",
    "    ansatz = IQPAnsatz({N:1, S:1}, n_layers=2, n_single_qubit_params=3)\n",
    "    for diagram in diagrams:\n",
    "    #Convert string Diagram to quantum circuit\n",
    "        circuit = ansatz(diagram)\n",
    "        circuits.append(circuit)\n",
    "        \n",
    "process_circuits(english_diagrams,english_circuits)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = AtomicType.NOUN\n",
    "S = AtomicType.SENTENCE\n",
    "ansatz = IQPAnsatz({N:1, S:1}, n_layers=2)\n",
    "circuit = ansatz(parser.sentence2diagram(\"Hi\"))\n",
    "circuit.draw(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 0.1\n",
    "SEED = 42\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import PennyLaneModel\n",
    "\n",
    "all_circuits = russian_circuits + english_circuits\n",
    "\n",
    "backend_config = {'backend': 'default.qubit'}\n",
    "model = PennyLaneModel.from_diagrams(all_circuits, probabilities=True, normalize=True,backend_config=backend_config)\n",
    "model.initialise_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "qml.default_config['qiskit.ibmq.ibmqx_token'] = 'my_API_token'\n",
    "qml.default_config.save(qml.default_config.path)\n",
    "backend_config = {'backend': 'qiskit.ibmq',\n",
    "                  'device': 'ibmq_manilia',\n",
    "                  'shots': 1000}\n",
    "q_model = PennyLaneModel.from_diagrams(all_circuits, probabilities=True, normalize=True, backend_config=backend_config)\n",
    "q_model = initialise_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_hat, y):\n",
    "    return (torch.argmax(y_hat, dim=1) ==\n",
    "            torch.argmax(y, dim=1)).sum().item()/len(y)\n",
    "\n",
    "def loss(y_hat, y):\n",
    "    return torch.nn.functional.mse_loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import PytorchTrainer\n",
    "\n",
    "trainer = PytorchTrainer(\n",
    "    model=model,\n",
    "    loss_function=loss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    epochs=EPOCHS,\n",
    "    evaluate_functions={'acc': acc},\n",
    "    evaluate_on_train=True,\n",
    "    use_tensorboard=False,\n",
    "    verbose='text',\n",
    "    seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
